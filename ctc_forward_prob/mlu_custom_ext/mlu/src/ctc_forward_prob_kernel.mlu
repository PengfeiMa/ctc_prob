#include <ctc_forward_prob.h>
#include <kernel.h>
#include <stdio.h>

#define TABLE_SIZE 66
#define TABLE_LENGTH 64

#define BINARY_ALIGN_NUM 64
#define BINARY_NRAM_SIZE (MAX_NRAM_SIZE + REM_FOR_STACK - 112 * 1024)
#define MAX_NRAM_USABLE_SIZE (BINARY_NRAM_SIZE - TABLE_SIZE * sizeof(int))

#define MAXIMUM_NUMBER (int)0x7fffffff
#define MINUS_ONE (int)0xffffffff

__nram__ char nram_buffer[MAX_NRAM_USABLE_SIZE];
__nram__ int table_buffer[TABLE_SIZE] = {0, MAXIMUM_NUMBER, MINUS_ONE, 0};


template <typename T>
__mlu_func__ void computeLogE(T *nram_dst, T *nram_src, int deal_num) {
  if (sizeof(T) == sizeof(float)) {
#if __BANG_ARCH__ >= 300
    int x2d = 0x3f317217;
    float rlog2e = *(float *)&x2d;
    __bang_log((float *)nram_dst, (float *)nram_src, deal_num);
    __bang_mul_scalar((float *)nram_dst, (float *)nram_dst, (float)rlog2e, deal_num);
#else
    __bang_active_loghp((float *)nram_dst, (float *)nram_src, deal_num);
#endif
  }
}

template <typename T>
__mlu_func__ void computeLogAddExp(char *nram_output,
                                   char *nram_input1,
                                   char *nram_input2,
                                   char *auxiliary_a,
                                   const int32_t deal_num) {
#if __BANG_ARCH__ > 300
  // deal inf data
  __bang_eq((T *)auxiliary_a, (T *)nram_input1, (T *)nram_input2, deal_num);
  __bang_float2int32((int *)auxiliary_a, (T *)auxiliary_a, deal_num, 0);
  __bang_lut_s32((int *)auxiliary_a, (int *)auxiliary_a, (int *)table_buffer, deal_num, TABLE_LENGTH);
  __bang_band((char *)auxiliary_a, (char *)auxiliary_a, (char *)nram_input2, deal_num * sizeof(T));

  __bang_eq_scalar((T *)auxiliary_a, (T *)auxiliary_a, INFINITY, deal_num);
  __bang_float2int32((int *)auxiliary_a, (T *)auxiliary_a, deal_num, 0);
  __bang_lut_s32((int *)auxiliary_a, (int *)auxiliary_a, (int *)table_buffer + 2, deal_num, TABLE_LENGTH);
  __bang_band((char *)auxiliary_a, (char *)auxiliary_a, (char *)nram_input2, deal_num * sizeof(T));

  // compute logaddexp
  __bang_sub((T *)auxiliary_a, (T *)nram_input1, (T *)auxiliary_a, deal_num);
  __bang_abs((T *)auxiliary_a, (T *)auxiliary_a, deal_num);
  __bang_mul_scalar((T *)auxiliary_a, (T *)auxiliary_a, -1, deal_num);
  __bang_exp((T *)auxiliary_a, (T *)auxiliary_a, deal_num);
  __bang_add_scalar((T *)auxiliary_a, (T *)auxiliary_a, 1.0, deal_num);

  computeLogE((T *)auxiliary_a, (T *)auxiliary_a, deal_num);

  __bang_maxequal((T *)nram_input2, (T *)nram_input1, (T *)nram_input2, deal_num);
  __bang_add((T *)nram_output, (T *)nram_input2, (T *)auxiliary_a, deal_num);
#endif
}


template <typename T>
__mlu_func__ void computeCTCForwardProb(char *nram_o0, char *nram_o1,
                                        char *nram_r0, char *nram_r1,
                                        char *nram_s,
                                        char *nram_x0, char *nram_x1,
                                        char *nram_aux,
                                        size_t num_per_compute) {
  /* ctc forward prob algorithm
    r[t][0] = torch.logaddexp(r[t-1][0], s[t-1])
    r[t][1] = torch.logaddexp(r[t-1][0], r[t-1][1])
    r[t] += x[t]
  */
  computeLogAddExp<T>(nram_o0, nram_r0, nram_s, nram_aux, num_per_compute);
  computeLogAddExp<T>(nram_o1, nram_r0, nram_r1, nram_aux, num_per_compute);
  __bang_add((T *)nram_o0, (T *)nram_o0, (T *)nram_x0, num_per_compute);
  __bang_add((T *)nram_o1, (T *)nram_o1, (T *)nram_x1, num_per_compute);
}


template <typename T>
__mlu_global__ void MLUBlockKernelCTCForwardProb(T *r,
                                                 T *s,
                                                 T *x,
                                                 T *output,
                                                 size_t start,
                                                 size_t end,
                                                 size_t num_per_compute) {
  /* nram space
    | r0[B*S] | r1[B*S] | ping_s[B*S] | pong_s[B*S] |
    | ping_x0[B*S] | pong_x0[B*S] | ping_x1[B*S] | pong_x1[B*S] |
    | o0[B*S] | o1[B*S] | aux[B*S] | ... |
  */
  size_t size_per_compute = num_per_compute * sizeof(T);
  size_t size_per_compute_aligned = CEIL_ALIGN(size_per_compute, BINARY_ALIGN_NUM);

  char *nram_r0 = nram_buffer;
  char *nram_r1 = nram_r0 + size_per_compute_aligned;
  char *nram_s = nram_r1 + size_per_compute_aligned;
  char *nram_x0 = nram_s + 2 * size_per_compute_aligned;
  char *nram_x1 = nram_x0 + 2 * size_per_compute_aligned;
  char *nram_o0 = nram_x1 + 2 * size_per_compute_aligned;
  char *nram_o1 = nram_o0 + size_per_compute_aligned;
  char *nram_aux = nram_o1 + size_per_compute_aligned;

  size_t repeat = end - start;

  if (repeat > 0) {
    __memcpy_async(nram_r0,
                   (char *)r + (start - 1) * 2 * size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __memcpy_async(nram_r1,
                   (char *)r + (start - 1) * 2 * size_per_compute + size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __memcpy_async(nram_s,
                   (char *)s + (start - 1) * size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __memcpy_async(nram_x0,
                   (char *)x + start * 2 * size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __memcpy_async(nram_x1,
                   (char *)x + start * 2 * size_per_compute + size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __asm__ volatile("sync;");
  }
  if (repeat > 1) {
    __memcpy_async(nram_s + size_per_compute_aligned,
                   (char *)s + start * size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __memcpy_async(nram_x0 + size_per_compute_aligned,
                   (char *)x + (start + 1) * 2 * size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __memcpy_async(nram_x1 + size_per_compute_aligned,
                   (char *)x + (start + 1) * 2 * size_per_compute + size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    computeCTCForwardProb<T>(nram_o0, nram_o1,
                             nram_r0, nram_r1,
                             nram_s,
                             nram_x0,
                             nram_x1,
                             nram_aux,
                             num_per_compute);
    __asm__ volatile("sync;");
  }
  for (size_t i = 0; i < repeat - 2; ++i) {
    // pvLock();
    if (i % 2 == 0) {
      __memcpy_async((char *)output + (start + i) * 2 * size_per_compute,
                     nram_o0,
                     size_per_compute,
                     NRAM2GDRAM);
      __memcpy_async((char *)output + (start + i) * 2 * size_per_compute + size_per_compute,
                     nram_o1,
                     size_per_compute,
                     NRAM2GDRAM);
    } else {
      __memcpy_async((char *)output + (start + i) * 2 * size_per_compute,
                     nram_r0,
                     size_per_compute,
                     NRAM2GDRAM);
      __memcpy_async((char *)output + (start + i) * 2 * size_per_compute + size_per_compute,
                     nram_r1,
                     size_per_compute,
                     NRAM2GDRAM);
    }
    // pvUnlock();
    __memcpy_async(nram_s + (i % 2) * size_per_compute_aligned,
                   (char *)s + (start + 1 + i) * size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __memcpy_async(nram_x0 + (i % 2) * size_per_compute_aligned,
                   (char *)x + (start + 2 + i) * 2 * size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    __memcpy_async(nram_x1 + (i % 2) * size_per_compute_aligned,
                   (char *)x + (start + 2 + i) * 2 * size_per_compute + size_per_compute,
                   size_per_compute,
                   GDRAM2NRAM);
    if (i % 2 == 0) {
      computeCTCForwardProb<T>(nram_r0, nram_r1,
                               nram_o0, nram_o1,
                               nram_s + size_per_compute_aligned,
                               nram_x0 + size_per_compute_aligned,
                               nram_x1 + size_per_compute_aligned,
                               nram_aux,
                               num_per_compute);
    } else {
      computeCTCForwardProb<T>(nram_o0, nram_o1,
                               nram_r0, nram_r1,
                               nram_s,
                               nram_x0,
                               nram_x1,
                               nram_aux,
                               num_per_compute);      
    }
    __asm__ volatile("sync;");
  }
  if (repeat > 1) {
    // pvLock();
    if ((repeat - 2) % 2 == 0) {
      __memcpy_async((char *)output + (start + repeat - 2) * 2 * size_per_compute,
                     nram_o0,
                     size_per_compute,
                     NRAM2GDRAM);
      __memcpy_async((char *)output + (start + repeat - 2) * 2 * size_per_compute + size_per_compute,
                     nram_o1,
                     size_per_compute,
                     NRAM2GDRAM);
    } else {
      __memcpy_async((char *)output + (start + repeat - 2) * 2 * size_per_compute,
                     nram_r0,
                     size_per_compute,
                     NRAM2GDRAM);
      __memcpy_async((char *)output + (start + repeat - 2) * 2 * size_per_compute + size_per_compute,
                     nram_r1,
                     size_per_compute,
                     NRAM2GDRAM);
    }
    // pvUnlock();
  }
  if (repeat > 0) {
    if ((repeat - 1) % 2 == 0) {
      computeCTCForwardProb<T>(nram_o0, nram_o1,
                               nram_r0, nram_r1,
                               nram_s,
                               nram_x0,
                               nram_x1,
                               nram_aux,
                               num_per_compute);
      __asm__ volatile("sync;");
    } else {
      computeCTCForwardProb<T>(nram_r0, nram_r1,
                               nram_o0, nram_o1,
                               nram_s + size_per_compute_aligned,
                               nram_x0 + size_per_compute_aligned,
                               nram_x1 + size_per_compute_aligned,
                               nram_aux,
                               num_per_compute);
      __asm__ volatile("sync;");
    }
    // pvLock();
    if ((repeat - 1) % 2 == 0) {
      __memcpy_async((char *)output + (start + repeat - 1) * 2 * size_per_compute,
                     nram_o0,
                     size_per_compute,
                     NRAM2GDRAM);
      __memcpy_async((char *)output + (start + repeat - 1) * 2 * size_per_compute + size_per_compute,
                     nram_o1,
                     size_per_compute,
                     NRAM2GDRAM);
    } else {
      __memcpy_async((char *)output + (start + repeat - 1) * 2 * size_per_compute,
                     nram_r0,
                     size_per_compute,
                     NRAM2GDRAM);
      __memcpy_async((char *)output + (start + repeat - 1) * 2 * size_per_compute + size_per_compute,
                     nram_r1,
                     size_per_compute,
                     NRAM2GDRAM);
    }
    // pvUnlock();
  }

}


template <typename T>
void ctc_forward_prob_kernel_entry(cnrtQueue_t queue, T *r, T *s, T *x, T *output,
                                   size_t start, size_t end, size_t num_per_compute) {
  cnrtDim3_t k_dim = {1, 1, 1};
  cnrtFunctionType_t k_type = cnrtFuncTypeBlock;

  // call mlu kernel
  MLUBlockKernelCTCForwardProb<<<k_dim, k_type, queue>>>(r, s, x, output, start, end, num_per_compute);

  cnrtQueueSync(queue);
}


template void ctc_forward_prob_kernel_entry(
    cnrtQueue_t, float *, float *, float *, float *, size_t, size_t, size_t
);